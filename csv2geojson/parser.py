import csv
import json
import pathlib
import shutil
from collections import OrderedDict
from urllib.parse import quote

import pandas as pd
import numpy as np
import logging
import logzero
from logzero import logger
from geojson import Feature, FeatureCollection, Point

from csv2geojson import util, genre

def normalize_and_geocode(row: pd.Series, pref_name: str, zip_code_validation=False):
    """
    ã‚¸ãƒ£ãƒ³ãƒ«åã¨ä½æ‰€ã‚’æ­£è¦åŒ–ã—ã€ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§å–å¾—ã—ãŸåœ°ç†æƒ…å ±ã¨åˆã‚ã›ã¦GeoJSONã®1Pointã«ç›¸å½“ã™ã‚‹pd.Seriresã‚’ç”Ÿæˆ
    """
    # ã‚¸ãƒ£ãƒ³ãƒ«å†åˆ†é¡
    try:
        genre_code = genre.classify(row['genre_name'])
        row['genre_code'] = genre_code
    except genre.GenreNotFoundError as e:
        logger.warning('{}: {}'.format(e, row.to_dict()))
        row['genre_code'] = genre.GENRE_ãã®ä»–

    # å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰latlngãŒæä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å„ªå…ˆã—ã¦ãã¡ã‚‰ã‚’åˆ©ç”¨
    # ã“ã®å ´åˆã¯ä½æ‰€ã®æ­£è¦åŒ–ã‚„ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã†å¿…è¦ãŒãªã„
    if row['provided_lat'] and row['provided_lng']:
        lat = float(row['provided_lat'])
        lng = float(row['provided_lng'])
        row['lat'] = lat
        row['lng'] = lng
        row['normalized_address'] = ''
        row['_dams_score'] = ''
        row['_dams_name'] = ''
        row['_dams_tail'] = ''
        row['_ERROR'] = np.nan

        # ãã®ä»–ã®è£œè¶³æƒ…å ±ã‚’è¿½åŠ 
        googlemap_q_string = '{} {}'.format(row['address'], row['shop_name'])
        row['google_map_url'] = 'https://www.google.com/maps/search/?q=' + quote(googlemap_q_string)
        row['_gsi_map_url'] = 'https://maps.gsi.go.jp/#17/{}/{}/'.format(lat, lng)

        return row

    # ä½æ‰€ã®æ­£è¦åŒ–ã€ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    try:
        address = row['address']
        normalized_address = util.normalize_for_pydams(address, pref_name)
        lat, lng, _debug = util.geocode_with_pydams(normalized_address)

        # éƒµä¾¿ç•ªå·ã§ã®ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµæœã«å¯¾ã™ã‚‹æ­£å½“æ€§ãƒã‚§ãƒƒã‚¯(ä»»æ„)
        if zip_code_validation:
            util.validate_by_zipcode(zip_code=row['zip_code'], address=normalized_address)
        # MEMO: ç°¡æ˜“çš„ãªã‚‚ã®ã§ã‚ã‚Šã€ã€Œæ„›çŸ¥çœŒåå¤å±‹å¸‚xxxxã€ã‚’ã€Œåå¤å±‹xxxxã€ã¨èª¤å…¥åŠ›ã•ã‚ŒãŸã“ã¨ã§ã€
        # åƒè‘‰çœŒæˆç”°å¸‚ã€æ–°æ½ŸçœŒä½æ¸¡å¸‚ã«ã‚ã‚‹åœ°åã®ã€Œåå¤å±‹ã€ã®ã‚ˆã†ã«ã€æ˜ã‚‰ã‹ã«èª¤ã£ãŸã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµæœã«ãªã£ã¦ã„ã‚‹å ´åˆã«
        # æ¤œå‡ºã§ãã‚‹ã€ã¨ã„ã†ç¨‹åº¦ã®ã‚‚ã®ã€‚posutoã‚’ç”¨ã„ã¦ã„ã‚‹ãŒã€çµæœã¯çœŒåã¾ã§ã—ã‹åˆ©ç”¨ã—ã¦ã„ãªã„ã€‚
        # (posuto.city, posuto.neighborhood ã®å€¤ã‚‚ä½¿ãˆã‚‹ã“ã¨ã¯ä½¿ãˆã‚‹ãŒã€pydamsãŒè¿”ã™ä½æ‰€å½¢å¼ã¨posutoãŒè¿”ã™çµæœã®ä½æ‰€å½¢å¼ãŒç•°ãªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€‚
        #  ä¾‹: pydams = "æ ƒæœ¨çœŒ/å¡©è°·éƒ¡/é«˜æ ¹æ²¢ç”º",  posuto = "æ ƒæœ¨çœŒ/é«˜æ ¹æ²¢ç”º"
        # ã©ã¡ã‚‰ã‚‚ä½æ‰€å½¢å¼ã¨ã—ã¦ã¯æ­£ã—ãã€ã©ã¡ã‚‰ã‚‚åŒã˜ä½æ‰€ã‚’æŒ‡ã™ãŒã€ã“ã†ã„ã£ãŸéƒ¡ã¨ã‹å­—(å¤§å­—)ã®æ­£è¦åŒ–ã¾ã§è€ƒãˆã‚‹ã¨é ­ãŒãŠã‹ã—ããªã£ã¦ãã‚‹ã®ã§)
        # ãªãŠä»»æ„ã¨ã—ã¦ã„ã‚‹ã®ã¯ã€posutoã®å®Ÿè£…ãŒå†…éƒ¨çš„ã«sqlite3ã‚’ä½¿ã£ã¦ãŠã‚Šã€ã¨ã«ã‹ãé…ã„ãŸã‚ã€‚
        # (æ ƒæœ¨ã®3500ä»¶ç¨‹åº¦ã§ off=1min, on=4.3min ãã‚‰ã„ã®å·®ãŒã¤ã)
        row['lat'] = lat
        row['lng'] = lng
        row['normalized_address'] = normalized_address
        row['_dams_score'] = _debug[0]
        row['_dams_name'] = _debug[1]
        row['_dams_tail'] = _debug[2]
        row['_ERROR'] = np.nan

        # ãã®ä»–ã®è£œè¶³æƒ…å ±ã‚’è¿½åŠ 
        googlemap_q_string = '{} {}'.format(normalized_address, row['shop_name'])
        row['google_map_url'] = 'https://www.google.com/maps/search/?q=' + quote(googlemap_q_string)
        row['_gsi_map_url'] = f'https://maps.gsi.go.jp/#17/{lat}/{lng}/'

    except (util.NormalizeError, util.GeocodeError) as e:
        # ä½æ‰€ã®æ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼(NormalizeError), ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼(GeocodeError)ãŒç™ºç”Ÿã—ãŸå ´åˆã€
        # dfã®_ERRORåˆ—ã«ç™ºç”Ÿã—ãŸã‚¨ãƒ©ãƒ¼åã‚’è¿½åŠ ã€å¾Œã‹ã‚‰è¿½ãˆã‚‹ã‚ˆã†ã«ã—ã¦ãŠã
        name = e.__class__.__name__
        row['_ERROR'] = name
        logger.warning('{}: {}'.format(name, row.to_dict()))
        logger.warning(e)
    except util.ZipCodeValidationError as e:
        # éƒµä¾¿ç•ªå·ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼(ZipCodeValidationError)ãŒç™ºç”Ÿã—ãŸå ´åˆã€
        # ä»¥ä¸‹ã®ã‚ˆã†ãªã‚±ãƒ¼ã‚¹ãŒã‚ã‚Šã€ä¸€æ¦‚ã«ã‚¨ãƒ©ãƒ¼ã§ã‚ã‚‹ã¨å‡¦ç†ã§ããªã„ã®ã§ã€ãƒ­ã‚°å‡ºåŠ›ã ã‘ã¨ã™ã‚‹
        # 1. ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«å¤±æ•—ã—ã¦ãŠã‚Šã€é–“é•ã£ãŸä½æ‰€ã§ã€latlngãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã‚‹ (æ­£è¦åŒ–ãŒä¸ååˆ† or ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«å¤±æ•—ã—ã¦ã„ã‚‹)
        #   => ã“ã®ã‚±ãƒ¼ã‚¹ã‚’æƒ³å®šã—ã¦å®Ÿè£…ã—ãŸãŒã€ã»ã¨ã‚“ã©ã¯å…¥åŠ›ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒä¸ååˆ†ãªã“ã¨(éƒµä¾¿ç•ªå·ã§è£œå®Œã§ãã‚‹åŒºåã‚„å¸‚åã‚’çœç•¥ã—ã¦ã„ã‚‹ãªã©)
        #      ã«èµ·å› ã—ã¦ãŠã‚Šã€ã‹ãªã‚Šé¢å€’ãã•ã„ (å¯¾å¿œã™ã‚‹å ´åˆã€æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã‚‚å«ã‚ã¦ä½æ‰€è‡ªä½“ã®ãã¡ã‚“ã¨ã—ãŸæ­£è¦åŒ–ãŒå¿…è¦ã«ãªã£ã¦ãã‚‹)
        # 2. éƒµä¾¿ç•ªå·ãŒãã‚‚ãã‚‚é–“é•ã£ã¦ã„ã‚‹ (å…ƒãƒ‡ãƒ¼ã‚¿ãŒæ‚ªã„) => ã©ã†ã—ã‚ˆã†ã‚‚ãªã„ã€ä½æ‰€ã®æ–¹ãŒæ­£ã—ã‘ã‚Œã°(ã¾ã‚)å•é¡Œã¯ãªã„
        # 3. éƒµä¾¿ç•ªå·ãŒãã‚‚ãã‚‚ã€Œè¤‡æ•°ã®éƒ½é“åºœçœŒã«ã¾ãŸãŒã‚‹ã€éƒµä¾¿ç•ªå·ã§ã‚ã‚‹ (ä¾‹: "498-0000") => éƒµä¾¿ç•ªå·ã®ä»•æ§˜ã€å•é¡Œã¯ãªã„
        name = e.__class__.__name__
        logger.warning('ğŸ“® {}: {}'.format(name, row.to_dict()))
        logger.warning(e)
    except Exception as e:
        # ãã‚Œä»¥å¤–ã®ä¾‹å¤–ãŒç™ºç”Ÿã—ãŸå ´åˆã«ã¯ã‚³ã‚±ã•ã›ã‚‹
        logger.error('{}: {}'.format(e.__class__.__name__, row.to_dict()))
        logger.exception(e)
        raise

    return row

def make_feature(row: pd.Series, debug=False):
    """
    GeoJSONã®Featureè¦ç´ (POINTé™å®š)ã‚’ä½œæˆ
    @see https://pypi.org/project/geojson/#point
    """
    try:
        # debugã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æœ‰ç„¡ã§prefixã«'_'ãŒã¤ã„ãŸé …ç›®ã‚’å‡ºã—åˆ†ã‘
        props = OrderedDict(row) if debug \
            else OrderedDict({k: v for k, v in OrderedDict(row).items() if not k.startswith('_') })

        # lat, lngã¯ã€Œproperitesã€ã§ã¯ãªãã€Œgeometryã€ã«é…ç½®
        lat = props.pop('lat')
        lng = props.pop('lng')
    except Exception as e:
        logger.error(props)
        raise

    coords = (lng, lat) # MEMO: lng, latã®é †ç•ªã«æ³¨æ„
    return Feature(geometry=Point(coords), properties=props)

def write_geojson(dest, df: pd.DataFrame, debug=False):
    """
    GeoJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    """
    # GeoJSONã®FeatureCollectionè¦ç´ , Featureè¦ç´ ã‚’ä½œæˆ
    # @see https://pypi.org/project/geojson/#featurecollection
    logger.debug('  ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°= {}'.format(len(df)))
    features = df.apply(make_feature, axis=1, debug=debug).tolist()
    feature_collection = FeatureCollection(features=features)
    with open(dest, 'w', encoding='utf-8') as f:
        if debug:
            # ãƒ‡ãƒãƒƒã‚°ç”¨GeoJSONã¯èª­ã¿ã‚„ã™ã„ã‚ˆã†ã«ã€prettyã—ã¦å‡ºåŠ›
            json.dump(feature_collection, f, ensure_ascii=False, indent=4)
        else:
            # æœ¬ç•ªç”¨GeoJSONã¯ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºå‰Šæ¸›ã®ãŸã‚ã€minifyã—ã¦å‡ºåŠ›
            json.dump(feature_collection, f, ensure_ascii=False, separators=(',', ':'))


class Csv2GeoJSON:
    # @see goto_eat_scrapy.settings.FEED_EXPORT_FIELDS
    FEED_EXPORT_FIELDS = ['shop_name', 'address', 'tel', 'genre_name', 'zip_code', 'official_page', 'opening_hours', 'closing_day', 'area_name', 'detail_page', 'provided_lat', 'provided_lng']
    NORMALIZED_CSV_EXPORT_FIELDS = FEED_EXPORT_FIELDS + [
        'lat',
        'lng',
        'normalized_address',
        'genre_code',
        'google_map_url',
        '_gsi_map_url',
        '_dams_score',
        '_dams_name',
        '_dams_tail',
    ]

    def __init__(self, src: pathlib.Path, zip_code_validation=False):
        self.zip_code_validation = zip_code_validation
        self._parse(src)

    def _parse(self, src: pathlib.Path):
        logger.debug(f'å…¥åŠ›CSV={src}')

        # CSVã®å…¨ã‚«ãƒ©ãƒ ã‚’æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€
        # MEMO: æ›¸å¼ã«ã‚ˆã£ã¦ã¯tel, zip_codeãªã©ãŒintã§èªè­˜ã•ã‚Œã‚‹(ä¾‹: "09012345678"ã€æ›¸å¼ã¯å„éƒ½é“åºœçœŒã®ã‚µã‚¤ãƒˆã«ä¾å­˜)
        # åŸºæœ¬çš„ã«crawlerå´ã§ã¯æ›¸å¼å¤‰æ›ã›ãšã€(å¿…è¦ã§ã‚ã‚Œã°)csv2geojsonå´ã§æ›¸å¼å¤‰æ›ã‚’è¡Œã†æ–¹é‡ã€‚
        # (ç¾çŠ¶ã¯ç‰¹ã«å‡¦ç†ã—ã¦ã„ãªã„ãŒã€å¿…è¦ãªã‚‰éƒµä¾¿ç•ªå·ã‚„é›»è©±ç•ªå·ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’çµ±ä¸€ã—ãŸã‚Šã—ã¦ã‚‚ã‚ˆã„)
        df = pd.read_csv(src, encoding="utf-8", dtype=str).fillna('')
        logger.debug('  å…¥åŠ›ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°= {}'.format(len(df)))

        # èª­ã¿è¾¼ã‚“ã CSVãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯(åº—å and ä½æ‰€)
        # ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯ã¨ã—ã¦ååˆ†ã¨ã¯è¨€ãˆãªã„ãŒã€å‚è€ƒç¨‹åº¦ã«
        # MEMO: ä»¥ä¸‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚‹ã®ã§ã€åº—åãŠã‚ˆã³ä½æ‰€å˜ä½“ã ã¨é‡è¤‡åˆ¤å®šã§ããªã„ã€‚
        # ãƒ»åŒã˜åº—åã®åˆ¥ã®åº—(ä¾‹: æ„›çŸ¥çœŒã®ã€Œã™ãšã‚„ã€)
        # ãƒ»åŒã˜ä½æ‰€ã®åˆ¥ã®åº—(ä¾‹: åŒã˜ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ«å†…)
        duplicated_records = df[df.duplicated(subset=['shop_name', 'address'])]
        if not duplicated_records.empty:
            # é‡è¤‡è¡Œã®å‰Šé™¤
            ## MEMO: ãƒ‡ãƒ¼ã‚¿é‡è¤‡ã¯(ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã«å®Ÿè£…ãƒŸã‚¹ãŒãªã‘ã‚Œã°)å…¬å¼ã‚µã‚¤ãƒˆå´ã®å•é¡Œãªã®ã§ã€é‡è¤‡å‰Šé™¤ã—ãŸã‚ã¨ã«å‡¦ç†ã‚’ç¶šè¡Œ
            df.drop_duplicates(subset=['shop_name', 'address'], keep='last', inplace=True)
        self.duplicated_df = duplicated_records

        # æ­£è¦åŒ–å‡¦ç†ã¨ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        # (å¤±æ•—ã—ãŸå ´åˆã¯_ERRORåˆ—ã«å€¤ãŒå…¥ã‚‹ã®ã§ã€ãã®è¡Œã¯ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦å‡¦ç†)
        logger.info(f'normalize...')
        df = df.apply(normalize_and_geocode, axis=1, pref_name=src.stem, zip_code_validation=self.zip_code_validation)
        self.error_df = df[df['_ERROR'].notnull()]                              # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
        self.normalized_df = df[df['_ERROR'].isnull()].drop(columns='_ERROR')   # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ä»¥å¤–ã‚’å–å¾—ã€_ERRORåˆ—ã¯å‰Šé™¤

    def write_normalized_csv(self, dest):
        """
        æ­£è¦åŒ–ã€åœ°ç†æƒ…å ±ç­‰ã‚’ä»˜ä¸ã—ãŸCSVã‚’å‡ºåŠ›
        """
        logger.info('create normalized_csv ...')
        self.normalized_df.to_csv(dest, columns=self.NORMALIZED_CSV_EXPORT_FIELDS, index=False)
        logger.debug('  ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°={}'.format(len(self.normalized_df)))

    def write_all_geojson(self, dest, debug=False):
        """
        ã‚¸ãƒ£ãƒ³ãƒ«åˆ†ã‘ãªã—ã§GeoJSONã‚’å‡ºåŠ›
        """
        logger.info('create all_geojson {} ...'.format('<_debug> ' if debug else ''))
        write_geojson(dest, self.normalized_df, debug)

    def write_genred_geojson(self, output_dir, debug=False):
        """
        ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ã§GeoJSONã‚’å‡ºåŠ›
        """
        for genre_code in sorted(self.normalized_df['genre_code'].unique()):
            outfile = output_dir / f'genre{genre_code}.geojson'
            df = self.normalized_df[self.normalized_df['genre_code'] == genre_code]
            logger.info('create genre{}_geojson {} ...'.format(genre_code, '<_debug> ' if debug else ''))
            write_geojson(outfile, df, debug)

    def write_error_json(self, dest):
        """
        ã‚¨ãƒ©ãƒ¼ç¢ºèªç”¨JSONã‚’å‡ºåŠ›
        """
        logger.info(f'create _error.json ...')
        logger.debug('  é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°= {}'.format(len(self.duplicated_df)))
        logger.debug('  ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°= {}'.format(len(self.error_df)))
        if len(self.duplicated_df) == 0 and len(self.error_df) == 0:
            # é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ã€ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã¨ã‚‚ã«å­˜åœ¨ã—ãªã„å ´åˆã€JSONå‡ºåŠ›ã—ãªã„
            return
        data = {
            'duplicated': self.duplicated_df.fillna('').to_dict(orient='records'),
            'error': self.error_df.fillna('').to_dict(orient='records'),
        }
        with open(dest, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)    # pretty

    def write_all(self, output_dir, cleanup=True):
        """
        å„ç¨®æˆæœç‰©ã®ä¸€æ‹¬ç”Ÿæˆ
        """
        # å‡ºåŠ›å…ˆã®æº–å‚™
        if cleanup:
            shutil.rmtree(output_dir, ignore_errors=True)
        output_dir_debug = output_dir / '_debug'
        output_dir_debug.mkdir(parents=True, exist_ok=True)

        self.write_normalized_csv(output_dir / 'normalized.csv')
        self.write_error_json(output_dir / '_error.json')
        self.write_all_geojson(output_dir / 'all.geojson')
        self.write_all_geojson(output_dir_debug / 'all.geojson', debug=True)
        self.write_genred_geojson(output_dir)
        self.write_genred_geojson(output_dir_debug, debug=True)


if __name__ == "__main__":
    # usage:
    # $ docker-compose run csv2geojson python -m csv2geojson.parser

    # GeoJSONã®Featureã‚’ä½œæˆã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«
    # addressã‹ã‚‰ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§lat,lngã‚’å–å¾—
    rawdata = pd.Series({
        'address': 'è¶³åˆ©å¸‚ä¸Šæ¸‹å‚ç”ºå­—ä¼Šå‹¢å®®364-1 ãªã‚“ã¨ã‹ãƒ“ãƒ«1F',
        'genre_name': 'ãƒ©ãƒ¼ãƒ¡ãƒ³ãƒ»é¤ƒå­',
        'official_page': 'https://www.kourakuen.co.jp/',
        'shop_name': 'å¹¸æ¥½è‹‘ è¶³åˆ©åº—',
        'tel': '0284-70-5620',
        'zip_code': '326-0335',
    })
    normalized_data = normalize_and_geocode(rawdata, pref_name='tochigi')
    feature = make_feature(normalized_data, debug=True)

    assert feature.properties['shop_name'] == 'å¹¸æ¥½è‹‘ è¶³åˆ©åº—'
    assert feature.properties['genre_code'] == genre.GENRE_éººé¡
    assert feature.properties['address'] == 'è¶³åˆ©å¸‚ä¸Šæ¸‹å‚ç”ºå­—ä¼Šå‹¢å®®364-1 ãªã‚“ã¨ã‹ãƒ“ãƒ«1F'
    assert feature.properties['normalized_address'] == 'æ ƒæœ¨çœŒè¶³åˆ©å¸‚ä¸Šæ¸‹å‚ç”ºå­—ä¼Šå‹¢å®®364-1'
    assert feature.properties['_dams_score'] == 5
    assert feature.properties['_dams_name'] == 'æ ƒæœ¨çœŒè¶³åˆ©å¸‚ä¸Šæ¸‹å‚ç”º'
    assert feature.properties['_dams_tail'] == 'ä¼Šå‹¢å®®364-1'
    assert feature.geometry.type == 'Point'
    assert feature.geometry.coordinates == [139.465439, 36.301109], "latlng did not match."

    print('success!!')
