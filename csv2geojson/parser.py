import json
import pathlib
import shutil
from collections import OrderedDict
from urllib.parse import quote

import pandas as pd
import numpy as np
from logzero import logger
from geojson import Feature, FeatureCollection, Point

from csv2geojson import util, genre, exceptions

# FIXME: ã‚„ã£ã¤ã‘å®Ÿè£…


def normalize_and_geocode(row: pd.Series, pref_name: str):
    """
    ã‚¸ãƒ£ãƒ³ãƒ«åã¨ä½æ‰€ã‚’æ­£è¦åŒ–ã—ã€ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§å–å¾—ã—ãŸåœ°ç†æƒ…å ±ã¨åˆã‚ã›ã¦GeoJSONã®1Pointã«ç›¸å½“ã™ã‚‹pd.Seriresã‚’ç”Ÿæˆ
    """
    # ã‚¸ãƒ£ãƒ³ãƒ«å†åˆ†é¡
    try:
        genre_code = genre.classify(row["genre_name"])
        row["genre_code"] = genre_code
    except genre.GenreNotFoundError as e:
        # ãƒ­ã‚°ã«å‡ºã™ã ã‘
        logger.info(e)
        logger.info("ğŸ´ {}: {}".format(e, row.to_dict()))
        row["genre_code"] = genre.GENRE_ãã®ä»–

    # latlngå–å¾—
    try:
        if row["provided_lat"] and row["provided_lng"]:
            # å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰latlngãŒæä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã¡ã‚‰ã‚’å„ªå…ˆã—ã¦åˆ©ç”¨(åƒè‘‰ã€ç¥å¥ˆå·ã€æ»‹è³€ãªã©)
            # ã“ã®å ´åˆã¯ä½æ‰€ã®æ­£è¦åŒ–ã‚„ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã†å¿…è¦ãŒãªã„
            lat = float(row["provided_lat"])
            lng = float(row["provided_lng"])
            row["normalized_address"] = ""
            row["_dams_score"] = ""
            row["_dams_name"] = ""
            row["_dams_tail"] = ""
            googlemap_q_string = "{} {}".format(row["address"], row["shop_name"])
        else:
            # ä½æ‰€ã‹ã‚‰ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§latlngã‚’æ±‚ã‚ã‚‹
            address = row["address"]
            normalized_address = util.normalize_for_pydams(address, pref_name)
            lat, lng, _dams_info = util.geocode_with_pydams(normalized_address)
            row["normalized_address"] = normalized_address
            row["_dams_score"] = _dams_info[0]
            row["_dams_name"] = _dams_info[1]
            row["_dams_tail"] = _dams_info[2]
            googlemap_q_string = "{} {}".format(normalized_address, row["shop_name"])

        row["lat"] = lat
        row["lng"] = lng
        row["google_map_url"] = "https://www.google.com/maps/search/?q=" + quote(
            googlemap_q_string
        )
        row["_ERROR"] = np.nan
        row["_WARNING"] = np.nan
        row["_gsi_map_url"] = f"https://maps.gsi.go.jp/#17/{lat}/{lng}/"

    except (exceptions.NormalizeError, exceptions.GeocodeError) as e:
        # _ERROR
        # ä½æ‰€ã®æ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼(NormalizeError), ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼(GeocodeError)ãŒç™ºç”Ÿã—ãŸå ´åˆã€
        # dfã®_ERRORåˆ—ã«ç™ºç”Ÿã—ãŸã‚¨ãƒ©ãƒ¼åã‚’è¿½åŠ ã€å¾Œã‹ã‚‰è¿½ãˆã‚‹ã‚ˆã†ã«ã—ã¦ãŠã
        name = e.__class__.__name__
        row["_ERROR"] = f"{name}({e})"
        logger.warning(e)
        logger.warning("ğŸ‘ {}: {}".format(name, row.to_dict()))

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
    try:
        util.validate(row)
    except (exceptions.ZipCodeValidationWarning, exceptions.ValidationWarning) as e:
        # _WARNING (ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼)
        name = e.__class__.__name__
        row["_WARNING"] = f"{name}({e})"
        logger.info(e)
        logger.info("â“ {}: {}".format(name, row.to_dict()))

    return row


def make_feature(row: pd.Series, debug=False):
    """
    GeoJSONã®Featureè¦ç´ (POINTé™å®š)ã‚’ä½œæˆ
    @see https://pypi.org/project/geojson/#point
    """
    try:
        # debugã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æœ‰ç„¡ã§prefixã«'_'ãŒã¤ã„ãŸé …ç›®ã‚’å‡ºã—åˆ†ã‘
        props = (
            OrderedDict(row)
            if debug
            else OrderedDict(
                {k: v for k, v in OrderedDict(row).items() if not k.startswith("_")}
            )
        )

        # lat, lngã¯ã€Œproperitesã€ã§ã¯ãªãã€Œgeometryã€ã«é…ç½®
        lat = props.pop("lat")
        lng = props.pop("lng")
    except Exception:
        logger.error(props, stack_info=True)
        raise

    coords = (lng, lat)  # MEMO: lng, latã®é †ç•ªã«æ³¨æ„
    return Feature(geometry=Point(coords), properties=props)


def write_geojson(dest, df: pd.DataFrame, debug=False):
    """
    GeoJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    """
    # GeoJSONã®FeatureCollectionè¦ç´ , Featureè¦ç´ ã‚’ä½œæˆ
    # @see https://pypi.org/project/geojson/#featurecollection
    logger.debug("  ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°= {}".format(len(df)))
    features = df.apply(make_feature, axis=1, debug=debug).tolist()
    feature_collection = FeatureCollection(features=features)
    with open(dest, "w", encoding="utf-8") as f:
        if debug:
            # ãƒ‡ãƒãƒƒã‚°ç”¨GeoJSONã¯èª­ã¿ã‚„ã™ã„ã‚ˆã†ã«ã€prettyã—ã¦å‡ºåŠ›
            json.dump(feature_collection, f, ensure_ascii=False, indent=4)
        else:
            # æœ¬ç•ªç”¨GeoJSONã¯ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºå‰Šæ¸›ã®ãŸã‚ã€minifyã—ã¦å‡ºåŠ›
            json.dump(feature_collection, f, ensure_ascii=False, separators=(",", ":"))


class Csv2GeoJSON:
    # @see goto_eat_scrapy.settings.FEED_EXPORT_FIELDS
    FEED_EXPORT_FIELDS = [
        "shop_name",
        "address",
        "tel",
        "genre_name",
        "zip_code",
        "official_page",
        "opening_hours",
        "closing_day",
        "area_name",
        "detail_page",
        "provided_lat",
        "provided_lng",
    ]
    NORMALIZED_CSV_EXPORT_FIELDS = FEED_EXPORT_FIELDS + [
        "lat",
        "lng",
        "normalized_address",
        "genre_code",
        "google_map_url",
        "_gsi_map_url",
        "_dams_score",
        "_dams_name",
        "_dams_tail",
    ]

    def __init__(self, src: pathlib.Path):
        self._parse(src)

    def _parse(self, src: pathlib.Path):
        logger.debug(f"å…¥åŠ›CSV={src}")

        # CSVã®å…¨ã‚«ãƒ©ãƒ ã‚’æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€
        # MEMO: æ›¸å¼ã«ã‚ˆã£ã¦ã¯tel, zip_codeãªã©ãŒintã§èªè­˜ã•ã‚Œã‚‹(ä¾‹: "09012345678"ã€æ›¸å¼ã¯å„éƒ½é“åºœçœŒã®ã‚µã‚¤ãƒˆã«ä¾å­˜)
        # åŸºæœ¬çš„ã«crawlerå´ã§ã¯æ›¸å¼å¤‰æ›ã›ãšã€(å¿…è¦ã§ã‚ã‚Œã°)csv2geojsonå´ã§æ›¸å¼å¤‰æ›ã‚’è¡Œã†æ–¹é‡ã€‚
        # (ç¾çŠ¶ã¯ç‰¹ã«å‡¦ç†ã—ã¦ã„ãªã„ãŒã€å¿…è¦ãªã‚‰éƒµä¾¿ç•ªå·ã‚„é›»è©±ç•ªå·ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’çµ±ä¸€ã—ãŸã‚Šã—ã¦ã‚‚ã‚ˆã„)
        df = pd.read_csv(src, encoding="utf-8", dtype=str).fillna("")
        logger.debug("  å…¥åŠ›ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°= {}".format(len(df)))

        # èª­ã¿è¾¼ã‚“ã CSVãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯(åº—å and ä½æ‰€)
        # ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯ã¨ã—ã¦ååˆ†ã¨ã¯è¨€ãˆãªã„ãŒã€å‚è€ƒç¨‹åº¦ã«
        # MEMO: ä»¥ä¸‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚‹ã®ã§ã€åº—åãŠã‚ˆã³ä½æ‰€å˜ä½“ã ã¨é‡è¤‡åˆ¤å®šã§ããªã„ã€‚
        # ãƒ»åŒã˜åº—åã®åˆ¥ã®åº—(ä¾‹: æ„›çŸ¥çœŒã®ã€Œã™ãšã‚„ã€)
        # ãƒ»åŒã˜ä½æ‰€ã®åˆ¥ã®åº—(ä¾‹: åŒã˜ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ«å†…)
        duplicated_records = df[df.duplicated(subset=["shop_name", "address"])]
        if not duplicated_records.empty:
            # é‡è¤‡è¡Œã®å‰Šé™¤
            ## MEMO: ãƒ‡ãƒ¼ã‚¿é‡è¤‡ã¯(ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã«å®Ÿè£…ãƒŸã‚¹ãŒãªã‘ã‚Œã°)å…¬å¼ã‚µã‚¤ãƒˆå´ã®å•é¡Œãªã®ã§ã€é‡è¤‡å‰Šé™¤ã—ãŸã‚ã¨ã«å‡¦ç†ã‚’ç¶šè¡Œ
            df.drop_duplicates(
                subset=["shop_name", "address"], keep="last", inplace=True
            )
        self.duplicated_df = duplicated_records

        # æ­£è¦åŒ–å‡¦ç†ã¨ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        # (å¤±æ•—ã—ãŸå ´åˆã¯_ERRORåˆ—ã«å€¤ãŒå…¥ã‚‹ã®ã§ã€ãã®è¡Œã¯ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦å‡¦ç†)
        logger.info(f"normalize...")
        df = df.apply(normalize_and_geocode, axis=1, pref_name=src.stem)
        self.error_df = df[df["_ERROR"].notnull()].drop(
            columns=["_WARNING"]
        )  # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
        self.warning_df = df[df["_WARNING"].notnull()].drop(
            columns=["_ERROR"]
        )  # ãƒ¯ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
        # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ä»¥å¤–ã‚’å–å¾—ã€_ERRORã¨_WARNINGåˆ—ã¯å‰Šé™¤
        # MEMO: _WARNINGã®ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã¯_error.jsonã«å‡ºã™ãŒã€GeoJSONã¨normalized_csvã«ã¯å‡ºåŠ›ã™ã‚‹
        self.normalized_df = df[df["_ERROR"].isnull()].drop(
            columns=["_ERROR", "_WARNING"]
        )

    def write_normalized_csv(self, dest):
        """
        æ­£è¦åŒ–ã€åœ°ç†æƒ…å ±ç­‰ã‚’ä»˜ä¸ã—ãŸCSVã‚’å‡ºåŠ›
        """
        logger.info("create normalized_csv ...")
        self.normalized_df.to_csv(
            dest, columns=self.NORMALIZED_CSV_EXPORT_FIELDS, index=False
        )
        logger.debug("  ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°={}".format(len(self.normalized_df)))

    def write_all_geojson(self, dest, debug=False):
        """
        ã‚¸ãƒ£ãƒ³ãƒ«åˆ†ã‘ãªã—ã§GeoJSONã‚’å‡ºåŠ›
        """
        logger.info("create all_geojson {} ...".format("<_debug> " if debug else ""))
        write_geojson(dest, self.normalized_df, debug)

    def write_genred_geojson(self, output_dir, debug=False):
        """
        ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ã§GeoJSONã‚’å‡ºåŠ›
        """
        for genre_code in sorted(self.normalized_df["genre_code"].unique()):
            outfile = output_dir / f"genre{genre_code}.geojson"
            df = self.normalized_df[self.normalized_df["genre_code"] == genre_code]
            logger.info(
                "create genre{}_geojson {} ...".format(
                    genre_code, "<_debug> " if debug else ""
                )
            )
            write_geojson(outfile, df, debug)

    def write_error_json(self, dest):
        """
        ã‚¨ãƒ©ãƒ¼ç¢ºèªç”¨JSONã‚’å‡ºåŠ›
        """
        logger.info(f"create _error.json ...")
        logger.debug("  é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°= {}".format(len(self.duplicated_df)))
        logger.debug("  ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°= {}".format(len(self.error_df)))
        logger.debug("  ãƒ¯ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°= {}".format(len(self.warning_df)))
        data = OrderedDict(
            {
                "duplicated": self.duplicated_df.fillna("").to_dict(orient="records"),
                "error": self.error_df.fillna("").to_dict(orient="records"),
                "warning": self.warning_df.fillna("").to_dict(orient="records"),
            }
        )
        with open(dest, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)  # pretty

    def write_all(self, output_dir, cleanup=True):
        """
        å„ç¨®æˆæœç‰©ã®ä¸€æ‹¬ç”Ÿæˆ
        """
        # å‡ºåŠ›å…ˆã®æº–å‚™
        if cleanup:
            shutil.rmtree(output_dir, ignore_errors=True)
        output_dir_debug = output_dir / "_debug"
        output_dir_debug.mkdir(parents=True, exist_ok=True)

        self.write_normalized_csv(output_dir / "normalized.csv")
        self.write_error_json(output_dir / "_error.json")
        self.write_all_geojson(output_dir / "all.geojson")
        self.write_all_geojson(output_dir_debug / "all.geojson", debug=True)
        self.write_genred_geojson(output_dir)
        self.write_genred_geojson(output_dir_debug, debug=True)


if __name__ == "__main__":
    # usage:
    # $ docker-compose run csv2geojson python -m csv2geojson.parser

    # GeoJSONã®Featureã‚’ä½œæˆã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«
    # addressã‹ã‚‰ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§lat,lngã‚’å–å¾—
    rawdata = pd.Series(
        {
            "address": "è¶³åˆ©å¸‚ä¸Šæ¸‹å‚ç”ºå­—ä¼Šå‹¢å®®364-1 ãªã‚“ã¨ã‹ãƒ“ãƒ«1F",
            "genre_name": "ãƒ©ãƒ¼ãƒ¡ãƒ³ãƒ»é¤ƒå­",
            "official_page": "https://www.kourakuen.co.jp/",
            "shop_name": "å¹¸æ¥½è‹‘ è¶³åˆ©åº—",
            "tel": "0284-70-5620",
            "zip_code": "326-0335",
        }
    )
    normalized_data = normalize_and_geocode(rawdata, pref_name="tochigi")
    feature = make_feature(normalized_data, debug=True)

    assert feature.properties["shop_name"] == "å¹¸æ¥½è‹‘ è¶³åˆ©åº—"
    assert feature.properties["genre_code"] == genre.GENRE_éººé¡
    assert feature.properties["address"] == "è¶³åˆ©å¸‚ä¸Šæ¸‹å‚ç”ºå­—ä¼Šå‹¢å®®364-1 ãªã‚“ã¨ã‹ãƒ“ãƒ«1F"
    assert feature.properties["normalized_address"] == "æ ƒæœ¨çœŒè¶³åˆ©å¸‚ä¸Šæ¸‹å‚ç”ºå­—ä¼Šå‹¢å®®364-1"
    assert feature.properties["_dams_score"] == 5
    assert feature.properties["_dams_name"] == "æ ƒæœ¨çœŒè¶³åˆ©å¸‚ä¸Šæ¸‹å‚ç”º"
    assert feature.properties["_dams_tail"] == "ä¼Šå‹¢å®®364-1"
    assert feature.geometry.type == "Point"
    assert feature.geometry.coordinates == [
        139.465439,
        36.301109,
    ], "latlng did not match."

    print("success!!")
